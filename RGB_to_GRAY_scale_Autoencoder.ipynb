{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshath\\AppData\\Local\\Continuum\\Anaconda3\\envs\\rl\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import savefig\n",
    "import cv2\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 3670"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3670, 128, 128, 3)\n",
      "(3670, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for i in range(1, num_images+1):\n",
    "    img = cv2.imread(\"color_images/color_\" +str(i) +\".jpg\" )\n",
    "    dataset.append(np.array(img))\n",
    "\n",
    "dataset_source = np.asarray(dataset)\n",
    "print(dataset_source.shape)\n",
    "\n",
    "dataset_tar = []\n",
    "\n",
    "for i in range(1, num_images+1):\n",
    "    img = cv2.imread(\"gray_images/gray_\" +str(i) +\".jpg\", 0)    \n",
    "    dataset_tar.append(np.array(img))\n",
    "\n",
    "dataset_target = np.asarray(dataset_tar)\n",
    "print(dataset_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_target = dataset_target[:, :, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(inputs): # Undercomplete Autoencoder\n",
    "    \n",
    "    # Encoder\n",
    "    \n",
    "    net = tf.layers.conv2d(inputs, 128, 2, activation = tf.nn.relu)\n",
    "    print(net.shape)\n",
    "    net = tf.layers.max_pooling2d(net, 2, 2, padding = 'same')\n",
    "    print(net.shape)\n",
    "\n",
    "    # Decoder\n",
    "    \n",
    "    net = tf.image.resize_nearest_neighbor(net, tf.constant([129, 129]))\n",
    "    net = tf.layers.conv2d(net, 1, 2, activation = None, name = 'outputOfAuto')\n",
    "\n",
    "    print(net.shape)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 127, 127, 128)\n",
      "(?, 64, 64, 128)\n",
      "(?, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "ae_inputs = tf.placeholder(tf.float32, (None, 128, 128, 3), name = 'inputToAuto')\n",
    "ae_target = tf.placeholder(tf.float32, (None, 128, 128, 1))\n",
    "\n",
    "ae_outputs = autoencoder(ae_inputs)\n",
    "lr = 0.001\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(ae_outputs - ae_target))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "# Intialize the network \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you don't want to train the network skip the cell righ below and dowload the pre-trained model. After downloading the pre-trained model run the cell below to the immediate below cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epoch_num = 50\n",
    "\n",
    "saving_path = 'K:/autoencoder_color_to_gray/SavedModel/AutoencoderColorToGray.ckpt'\n",
    "\n",
    "saver_ = tf.train.Saver(max_to_keep = 3)\n",
    "\n",
    "batch_img = dataset_source[0:batch_size]\n",
    "batch_out = dataset_target[0:batch_size]\n",
    "\n",
    "num_batches = num_images//batch_size\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for ep in range(epoch_num):\n",
    "    batch_size = 0\n",
    "    for batch_n in range(num_batches): # batches loop\n",
    "\n",
    "        _, c = sess.run([train_op, loss], feed_dict = {ae_inputs: batch_img, ae_target: batch_out})\n",
    "        print(\"Epoch: {} - cost = {:.5f}\" .format((ep+1), c))\n",
    "            \n",
    "        batch_img = dataset_source[batch_size: batch_size+32]\n",
    "        batch_out = dataset_target[batch_size: batch_size+32]\n",
    "            \n",
    "        batch_size += 32\n",
    "    \n",
    "    saver_.save(sess, saving_path, global_step = ep)\n",
    "recon_img = sess.run([ae_outputs], feed_dict = {ae_inputs: batch_img})\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from K:/autoencoder_color_to_gray/SavedModel/AutoencoderColorToGray.ckpt-49\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "saver.restore(sess, 'K:/autoencoder_color_to_gray/SavedModel/AutoencoderColorToGray.ckpt-49')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "import glob as gl \n",
    "\n",
    "filenames = gl.glob('flower_images/*.png')\n",
    "\n",
    "test_data = []\n",
    "for file in filenames[0:100]:\n",
    "    test_data.append(np.array(cv2.imread(file)))\n",
    "\n",
    "test_dataset = np.asarray(test_data)\n",
    "print(test_dataset.shape)\n",
    "\n",
    "# Running the test data on the autoencoder\n",
    "batch_imgs = test_dataset\n",
    "gray_imgs = sess.run(ae_outputs, feed_dict = {ae_inputs: batch_imgs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "print(gray_imgs.shape)\n",
    "\n",
    "for i in range(gray_imgs.shape[0]):\n",
    "    cv2.imwrite('gen_gray_images/gen_gray_' +str(i) +'.jpeg', gray_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
